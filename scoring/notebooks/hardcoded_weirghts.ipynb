{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Importance (Q01, Q04, Q10):\n",
    "- Q01 (0.13 - Similar Reviews): Reviews that are very similar to each other often suggest a pattern of fake reviews, possibly generated by bots. Hence, it gets a strong weight.\n",
    "- Q04 (0.15 - Off-Topic Review): A review that doesn't discuss the product/service is a significant red flag for fake reviews, thus receiving a high weight.\n",
    "- Q10 (0.16 - Generic Review): Generic reviews often lack genuine insight and are common in automated or spam reviews, deserving a high weight.\n",
    "\n",
    "# Moderately High Importance (Q05, Q09):\n",
    "- Q05 (0.11 - Low Rating with Positive Text): Inconsistent narratives in reviews can indicate manipulation, so this gets a substantial weight.\n",
    "- Q09 (0.10 - Praising a Competitor): A review that seems to favor a competitor might be an attempt to subtly endorse another product, indicating potential bias or fakery.\n",
    "\n",
    "# Moderate Importance (Q06, Q08):\n",
    "- Q06 (0.08 - Extreme Emotions): Extreme emotions, such as excessive praise or criticism, can be indications of inauthentic reviews.\n",
    "- Q08 (0.08 - Word Overuse): Overusing specific keywords can be a tactic used to boost search or algorithmic visibility, thus suggesting possible manipulation.\n",
    "\n",
    "# Lower Importance (Q02, Q03, Q07, Q11):\n",
    "- Q02 (0.03 - Focus on Scenery): While focusing too much on scenery might indicate a lack of genuine engagement, it's a subtler sign, thus a lower weight.\n",
    "- Q03 (0.05 - Grammar): Good grammar can slightly suggest authenticity, but fake reviews can also be well-written, so it's of moderate low importance.\n",
    "- Q07 (0.05 - Generic Profile Names): Generic or random names might suggest anonymity or mass account creation, but itâ€™s not a definitive sign by itself.\n",
    "- Q11 (0.06 - Varied Length): The length of a review can vary naturally, but consistently unusual lengths can be suspicious, thus assigned a moderate weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = {\n",
    "    \"a01\": 0.13,  # Similarity with others\n",
    "    \"a02\": 0.03,  # Focus on scenery\n",
    "    \"a03\": 0.05,  # Grammatical correctness\n",
    "    \"a04\": 0.15,  # Relevance to the product\n",
    "    \"a05\": 0.11,   # Low rating but positive text\n",
    "    \"a06\": 0.08,   # Extreme emotions\n",
    "    \"a07\": 0.05,  # Generic profile name\n",
    "    \"a08\": 0.08,  # Overusing words\n",
    "    \"a09\": 0.1,   # Praising competitor\n",
    "    \"a10\": 0.06,   # Generic review\n",
    "    \"a11\": 0.04   # Varied length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_review_score(confidences):\n",
    "    weights = {\n",
    "        \"a01\": 0.05,  # Similarity with others\n",
    "        \"a02\": 0.03,  # Focus on scenery\n",
    "        \"a03\": 0.15,  # Grammatical correctness\n",
    "        \"a04\": 0.25,  # Relevance to the product\n",
    "        \"a05\": 0.1,   # Low rating but positive text\n",
    "        \"a06\": 0.1,   # Extreme emotions\n",
    "        \"a07\": 0.05,  # Generic profile name\n",
    "        \"a08\": 0.05,  # Overusing words\n",
    "        \"a09\": 0.1,   # Praising competitor\n",
    "        \"a10\": 0.1,   # Generic review\n",
    "        \"a11\": 0.04   # Varied length\n",
    "    }\n",
    "\n",
    "    # Calculate the total weighted score\n",
    "    total_weighted_score = sum(weight * confidences[question] for question, weight in weights.items())\n",
    "    \n",
    "    # Return normalized score (total_weighted_score already reflects the proportions)\n",
    "    return total_weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated review score is: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "confidences = {\n",
    "    \"a01\": 0.9,\n",
    "    \"a02\": 0.95,\n",
    "    \"a03\": 0.99,\n",
    "    \"a04\": 0.98,\n",
    "    \"a05\": 0.96,\n",
    "    \"a06\": 0.9,\n",
    "    \"a07\": 0.85,\n",
    "    \"a08\": 0.9,\n",
    "    \"a09\": 0.98,\n",
    "    \"a10\": 0.95,\n",
    "    \"a11\": 0.9\n",
    "}\n",
    "\n",
    "score = calculate_review_score(confidences)\n",
    "print(f'The calculated review score is: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated review score is: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "confidences = {\n",
    "    \"a01\": 0.9,\n",
    "    \"a02\": 0.75,\n",
    "    \"a03\": 0.85,\n",
    "    \"a04\": 0.98,\n",
    "    \"a05\": 0.9,\n",
    "    \"a06\": 0.9,\n",
    "    \"a07\": 0.8,\n",
    "    \"a08\": 0.98,\n",
    "    \"a09\": 0.9,\n",
    "    \"a10\": 0.9,\n",
    "    \"a11\": 0.95\n",
    "}\n",
    "\n",
    "score = calculate_review_score(confidences)\n",
    "print(f'The calculated review score is: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated review score is: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "confidences = {\n",
    "    \"a01\": 0.1,\n",
    "    \"a02\": 0.75,\n",
    "    \"a03\": 0.85,\n",
    "    \"a04\": 0.98,\n",
    "    \"a05\": 0.9,\n",
    "    \"a06\": 0.9,\n",
    "    \"a07\": 0.8,\n",
    "    \"a08\": 0.98,\n",
    "    \"a09\": 0.9,\n",
    "    \"a10\": 0.9,\n",
    "    \"a11\": 0.95\n",
    "}\n",
    "\n",
    "score = calculate_review_score(confidences)\n",
    "print(f'The calculated review score is: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated review score is: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "confidences = {\n",
    "    \"a01\": 0.9,\n",
    "    \"a02\": 0.7,\n",
    "    \"a03\": 0.85,\n",
    "    \"a04\": 0.4,\n",
    "    \"a05\": 0.9,\n",
    "    \"a06\": 0.1,\n",
    "    \"a07\": 0.8,\n",
    "    \"a08\": 0.18,\n",
    "    \"a09\": 0.9,\n",
    "    \"a10\": 0.9,\n",
    "    \"a11\": 0.95\n",
    "}\n",
    "\n",
    "score = calculate_review_score(confidences)\n",
    "print(f'The calculated review score is: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-scoring)",
   "language": "python",
   "name": "ml_score"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
